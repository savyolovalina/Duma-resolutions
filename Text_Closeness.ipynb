{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "full = np.load('full.npy')\n",
    "full300 = np.load('full300.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Препроцессинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text:str) -> str:\n",
    "    a = re.sub('\\r', ' ', text)\n",
    "    a = re.sub('\\x0b', ' ', a)\n",
    "    a = re.sub('\\t', ' ', a)\n",
    "    a = re.sub('\\xa0', ' ', a)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stops = stopwords.words('russian')\n",
    "stops.extend([' ', 'президент', 'закон', 'часть', 'абзац', 'федерация', 'российский', 'депутат', 'правительство', \n",
    "              'внесение', 'изменение', 'кодекс', 'статья', 'внести', 'изменить', 'законодательство'])\n",
    "\n",
    "punctuation = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~№»«'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "def preprocess(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Данная функция принимает на вход текст, \n",
    "    а возвращает тот же текст, но с пробелами между каждым токеном\n",
    "    \"\"\"\n",
    "    fast_clean = ' '.join(tokenizer.tokenize(clean_text(text.lower())))\n",
    "    for i in punctuation:\n",
    "        fast_clean = fast_clean.replace(i, ' ')\n",
    "    for i in '1234567890':\n",
    "        fast_clean = fast_clean.replace(i, ' ')\n",
    "    fast_clean = fast_clean.replace(' ст ', ' ')\n",
    "    fast_clean = fast_clean.replace('n', ' ')\n",
    "    clean = fast_clean.replace('\\x07', ' ')\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'вносится правительством российской федерации проект федеральный закон о внесении изменений в статьи     и     налогового кодекса российской федерации статья   внести в налоговый кодекс российской федерации   собрание законодательства российской федерации                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        следующие изменения       подпункт    пункта   статьи     изложить в следующей редакции          средства   которые получены объединением туроператоров в сфере выездного туризма   созданным в соответствии с федеральным законом от    ноября      года         фз   об основах туристской деятельности в российской федерации    в виде взносов   перечисленных в резервный фонд объединения туроператоров в сфере выездного туризма и в фонды персональной ответственности туроператоров в сфере выездного туризма   предназначенные для финансирования предусмотренных указанным федеральным законом расходов на оказание экстренной помощи туристам и на возмещение реального ущерба   возникшего в результате неисполнения туроператором обязательств по договору о реализации туристского продукта в сфере выездного туризма         пункт      статьи     изложить в следующей редакции            понесенные объединением туроператоров в сфере выездного туризма за счет средств резервного фонда объединения туроператоров в сфере выездного туризма и фондов персональной ответственности туроператоров в сфере выездного туризма   созданных в соответствии с федеральным законом от    ноября      года         фз   об основах туристской деятельности в российской федерации      статья   настоящий федеральный закон вступает в силу не ранее чем по истечении одного месяца со дня его официального опубликования и не ранее     го числа очередного налогового периода по соответствующему налогу   президент российской федерации'"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(full[510][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Векторизация документов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Лемматизатор (не учитывает контекст, как mystem, но подхдит для пользователей слабенькой винды)\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "pymorphy2_analyzer = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для двух списков документов \n",
    "tokens = {} \n",
    "for doc in full[:, 0]:\n",
    "    doc = preprocess(doc)\n",
    "    for token in doc.split(' '):\n",
    "        token = pymorphy2_analyzer.parse(token)[0].normal_form\n",
    "        if token not in stops: \n",
    "            if token not in tokens.keys():\n",
    "                tokens[token] = 1\n",
    "            else:\n",
    "                tokens[token] += 1\n",
    "                \n",
    "tokens = {} \n",
    "for doc in full300[:, 0]:\n",
    "    doc = preprocess(doc)\n",
    "    for token in doc.split(' '):\n",
    "        token = pymorphy2_analyzer.parse(token)[0].normal_form\n",
    "        if token not in stops:\n",
    "            if token not in tokens.keys():\n",
    "                tokens[token] = 1\n",
    "            else:\n",
    "                tokens[token] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для классификации документов с такими близкими текстами не подошли готовые векторизаторы, поэтому надо экспериментировать с частотностью слов в словаре-векторе. Параметры min и max показывают наименьшие и наибольшие частотности слов в словаре."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 8)"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# пример: от топ-2000 до топ-3000 по частотности \n",
    "most_common = {}\n",
    "for (key, value) in sorted(tokens.items(), key=lambda x: x[1])[-3000:-2000]: \n",
    "    most_common[key] = value\n",
    "min(most_common.values()), max(most_common.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def text_to_bow(text: str) -> np.array:\n",
    "    \"\"\"\n",
    "    Возвращает вектор, где для каждого слова из most_common\n",
    "    указано количество его употреблений\n",
    "    input: строка\n",
    "    output: вектор размерности словаря\n",
    "    \"\"\"\n",
    "    t = dict(zip(most_common.keys(), np.zeros(len(most_common)))) \n",
    "    for token in text.split(' '):\n",
    "        if token in t.keys():\n",
    "            t[token] += 1\n",
    "    return list(t.values())\n",
    "\n",
    "def items_to_bow(items: np.array, dictionary) -> np.array:\n",
    "    \"\"\" Для каждого дока возвращает вектор его bow\"\"\"\n",
    "    def string_to_bow(text: str) -> np.array:\n",
    "        \"\"\"\n",
    "        Возвращает вектор, где для каждого слова из most_common\n",
    "        указано количество его употреблений\n",
    "        input: строка из items \n",
    "        output: вектор размерности most_common \n",
    "        \"\"\"\n",
    "        t = dict(zip(dictionary.keys(), np.zeros(len(dictionary)))) \n",
    "        for token in text.split(' '):\n",
    "            if token in t.keys():\n",
    "                t[token] += 1\n",
    "        return list(t.values())\n",
    "    bows = []\n",
    "    for i in items[:, 0]:\n",
    "        i = preprocess(clean_text(i))\n",
    "        bows.append(string_to_bow(i))\n",
    "    return np.array(bows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(714, 1000)"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_bow = items_to_bow(full, most_common)\n",
    "docs_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(278, 1000)"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_bow300 = items_to_bow(full300, most_common)\n",
    "docs_bow300.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Поиск близких документов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Второе, с чем нужно экспериментировать, - количество слов, общих для двух документов (common_words). Логика такая: если умеренно редкие слова употребляются одинаковое или почти одинаковое количество раз в двух документах, то они, вероятно, об одном. Лучше сработавшая тактика - оставлять в most_common слова конкретной частоты: \n",
    "\n",
    "*min(most_common.values()) = max(most_common.values()) = 8*  \n",
    "\n",
    "и указывать конкретное значение common_words, равное половине выбранной частотности (в данном примере 8:2 = 4).\n",
    "\n",
    "Это будет значить, что всего в массиве текстов слово встретилось 8 раз, при этом 4 раза в одном документе и 4 - в другом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720839-7.doc 40551.docx\n",
      "895550-7.doc 91084.docx\n",
      "895550-7.doc 91084.docx\n"
     ]
    }
   ],
   "source": [
    "id300 = -1\n",
    "common_words = 4\n",
    "for text300 in docs_bow300:\n",
    "    id300 += 1\n",
    "    id700 = -1\n",
    "    for text in docs_bow:\n",
    "        id700 += 1\n",
    "        for i in range(len(most_common)):\n",
    "            if (text300[i]) & (text[i] == common_words):\n",
    "                print(full[id700][1], full300[id300][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
